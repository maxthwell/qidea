包含以下内容
分别使用强化学习算法玩一个大家都喜闻乐见的游戏：扎金花。
该问题的难度：
-------------------------------------------------------------------------------------------------------------------------------------------
	                         alphago                               扎金花
-------------------------------------------------------------------------------------------------------------------------------------------
参与人数：                         2                                    2~6
-------------------------------------------------------------------------------------------------------------------------------------------
每一步可选动作                   361-nSteps                             15               
-------------------------------------------------------------------------------------------------------------------------------------------
完成一局游戏的步数               >100                                   <20
-------------------------------------------------------------------------------------------------------------------------------------------
信息透明度               可以观测自身与对手的所有状态量           不能观察到其他对手的状态
-------------------------------------------------------------------------------------------------------------------------------------------
马尔科夫过程             是，当前动作只取决于最近8步的状态        否，当前动作不仅取决于当前状态，还取决于历史动作等，但可以转换为MDP
				（打劫的可能性）
--------------------------------------------------------------------------------------------------------------------------------------------
合作博弈                        否                                可能存在，但是会约束各个玩家独立。
----------------------------------------------------------------------------------------------------------------------------------------------------------------
MCTS                     由于围棋可以使用rollout对策略进行提升    不能基于当前局面进行rollout，因为不知道其他玩家手牌。
----------------------------------------------------------------------------------------------------------------------------------------------------------------
策略提升                       MCTS+A3C                               策略梯度法(因为无法看到对方手牌所以无法使用MCTS进行rollout)
----------------------------------------------------------------------------------------------------------------------------------------------------------------
难点                     动作空间的宽度与深度                信息不透明,存在对状态的长期依赖(可以直接把历史状态也作为模型输入的一部分)。
----------------------------------------------------------------------------------------------------------------------------------------------------------------

通过以上对比可以看出，扎金花是一个比围棋更加简单的游戏，其动作空间的宽度与深度不如围棋，主要难点在于其是一个多人博弈系统，而围棋仅仅是一个二人博弈系统。


扎金花游戏的当前状态有以下属性：
	c1)玩家个数N
	c2所有玩家的历史动作集。
	c3)当前玩家的手牌。
	c4)当前玩家曾经的比牌对象的手牌
	c5)被淘汰的玩家组成的标志向量
	c6)当前桌面上的玩家组成的标志向量
	c7)当前桌面上总共累积的注数
	c8)当前玩家的id
	c9)当前的轮数
	c10)剩余的轮数
	c11)当前允许押注的最小暗注（明注是暗注的两倍）
	c12)所有玩家的id与座次号。(构建一个N*N的矩阵，其每一行每一列都具有one-hot形式)
	以上状态有一部分冗余的信息，如根据c5,c6可以推导出c1，但是这没关系，因为我们会使用神经网络模型去除冗余信息,保留有用的特征。
	以上信息需要通过一些特殊的数据处理手段进行参数化与张量化，并且将参数限制在（-1，1）之间。如当前的轮数用一个20维的one-hot向量标识，向量的第几个维度为1就代表第几轮。

可以使用的动作：
	无操作(用于看牌阶段选择不看牌)
	看牌（在下注、弃牌，动作之前进行选择是否看牌）
	弃牌
	比牌0	(与第0个玩家比牌)
	比牌1
	比牌2
	比牌3
	比牌4
	比牌5
	下注1  (押1,指暗注，明注要下暗注的两倍即要押2)
	下注2  (下2注)
	下注4  (下4注)
	下注8  (下8注)
	下注16  (下16注)
	下注32  (下32注)
	每一个动作对应一个15维的one-hot向量。

策略模型：
	构造有12个输入张量的策略模型，分别输入以上属性，输出各个动作的概率分布,模型的超参数为theata。

奖励:
	下注就惩罚当前玩家所押的注数。
	比牌就惩罚比牌者当前允许最小注数的两倍。
	游戏结束，把奖励池中的所有注平均给所有的赢家。	

	

游戏结束条件：
	除了一个玩家外，其他的都弃牌或者比牌出局。

玩法：
	s1)发牌
	s2)所有的玩家可以选择执行看牌操作
	s3)玩家1从（下注、比牌、弃牌）中选择一个动作执行，并获取相应的游戏状态，奖励，是否结束等信息。
	s4)重复s2-s3直到桌面上只有一个玩家或者到达最大允许的轮数。
	s5)奖励所有的胜者。

规则:
	r1)前N轮不允许比牌。
	r2)每一个玩家不知道对方的手牌。
	r3)选择看牌后不允许再执行看牌动作
	r4)在看牌阶段如果选择了下注、比牌、弃牌等动作直接被淘汰出局
	r5)下注阶段执行了看牌操作的用户直接out。
	r6)前几局游戏对当前游戏无影响。（而人会收到前面几局的影响，造成对当前游戏局面的误判）

策略梯度法:
	利用策略梯度法进行回合更新，一局游戏代表一个回合。
	每玩一局实际上得到了6个样本，因为可以站在6个不同玩家角度，得到6组轨迹,根据6个玩家的在各个轨迹上的收益对相应的策略参数进行调节。
	策略：PI(a|s,theata)
		theata=>策略的参数。
		s=>玩家可以观测的状态。
		a=>动作。
		输出=>在已知调节s与theata下，采取不同动作a的概率分布（输出是一个归一化的条件概率，用softmax函数激活）。
		回合更新：
			站在一个玩家的角度，在一个回合中他所能观察到的状态与所采取的动作构成一条轨迹Trail。
			Trail={(s0,a0),(s1,a1),(s2,a2),(s3,a3).....(sT,aT)}
			完成一局游戏后的收益为R。
			则theata的更新方式如下:
			theata += alpha * sum(DELTA_theata(log(PI(ai|si,theata))) * A(ai,si))
			alpha为学习率
			为了提高学习率，可以将alpha设置一个比较大的数，但又需要控制策略更新的幅度，所以一般加上一个损失项KL(PI_old,PI_new)
			KL表示两个分布的KL散度，用于衡量两个概率分布的差异。
		具体算法：
			initialize theata_old, theata_new
			for i in range(N) do
				theata_old=theata_new
				#利用所有玩家使用PI_old策略玩的M局游戏构成的样本进行学习，修改策略参数
				play games for M times use PI_old, will get some trails for all player => {trails}
				for trail_k in {trails} do
					#利用策略梯度法，向着能获取到最大期望收益的方向做出调整。
					delta_theata += sum(DELTA_theata(A(ai,si)*log(PI(ai|si,theata))))
					s.b. to (ai,si) in trail_k ; theata = theata_old
				end for
				theata_new = theata_old + alpha * delta_theata/(M*count(players))
				#利用如果调整幅度过大，进行相应的惩罚,降低调整前后的两个策略的KL距离。
				theata_new -= belta * DELTA_theata(KL(PI(a|si,theata_old),PI(a|si,theata)))
				s.b. to theata = theata_new
			end for



为什么不使用AC算法：
	由于alphago使用了AC算法，战胜了人类职业棋手，为什么不使用AC算法呢？
	由于扎金花游戏的奖励与惩罚信息还算比较及时，每执行两次动作就能得到一次奖励信息，因此可以构造价值网络与策略网，因此也可以使用AC算法。
	AC算法很通用，但通用本身也说明它忽略了不同游戏的一些特点,例如扎金花一般都在20步以内结束,大多数局面3、5轮就结束了，而围棋动辄就要走上上百步。
	AC算法引入了价值网用于对当前策略进行评估，引入价值网的是为了解决完成一次任务（游戏）的步骤太多导致难以获取当前局面的价值，因此引入价值网络进行进行估计。
	但是扎金花由于其深度较浅的特点，完成一局后，可以很迅速地计算出每一个玩家的每一步的价值，引入价值网络反而增加了问题的复杂度,训练AC模型的周期远大于简单的PG。
	杀鸡焉用牛刀，AC算法还是用于复杂度更高的模型,扎金花这种简单的游戏还是交给策略梯度这种低级的算法吧。


用python编写一个模拟扎金花的游戏，默认玩家使用完全随机策略，玩10000局只需要2秒。后期会将策略网络添加进去，并进行训练。
