包含以下内容
分别使用DQN、PPO算法进行强化学习，玩一个大家都喜闻乐见的游戏：扎金花。

扎金花游戏的当前状态有以下属性：
	c1)玩家个数N
	c2所有玩家的历史动作集。
	c3)当前玩家的手牌。
	c4)当前玩家曾经的比牌对象的手牌
	c5)被淘汰的玩家组成的标志向量
	c6)当前桌面上的玩家组成的标志向量
	c7)当前桌面上总共累积的注数
	c8)当前玩家的id
	c9)当前的轮数
	c10)剩余的轮数
	c11)当前允许押注的最小暗注（明注是暗注的两倍）
	c12)所有玩家的id与座次号。(构建一个N*N的矩阵，其每一行每一列都具有one-hot形式)
	以上状态有一部分冗余的信息，如根据c5,c6可以推导出c1，但是这没关系，神经网络模型本身拥有去除冗余信息的能力。
	以上信息需要通过一些特殊的数据处理手段进行参数化与张量化，并且将参数限制在（-1，1）之间。如当前的轮数用一个20维的one-hot向量标识，向量的第几个维度为1就代表第几轮。

可以使用的动作：
	无操作(用于看牌阶段选择不看牌)
	看牌（在下注、弃牌，动作之前进行选择是否看牌）
	弃牌
	比牌0
	比牌1
	比牌2
	比牌3
	比牌4
	比牌5
	下注0
	下注1
	下注2
	下注3
	下注4
	下注5
	每一个动作对应一个15维的one-hot向量。

奖励:
	下注就惩罚当前玩家相应的注数。
	比牌就惩罚比牌者当前允许最小注数的两倍。
	游戏结束，把奖励池中的所有注平均给所有的赢家。	

	

游戏结束条件：
	除了一个玩家外，其他的都弃牌或者比牌出局。

玩法：
	s1)发牌
	s2)所有的玩家可以选择执行看牌操作
	s3)玩家1从（下注、比牌、弃牌）中选择一个动作执行，并获取相应的游戏状态，奖励，是否结束等信息。
	s4)重复s2-s3直到桌面上只有一个玩家或者到达最大允许的轮数。
	s5)奖励所有的胜者。

规则:
	r1)前N轮不允许比牌。
	r2)每一个玩家不知道对方的手牌。
	r3)选择看牌后不允许再执行看牌动作
	r4)在看牌阶段如果选择了下注、比牌、弃牌等动作直接被淘汰出局
	r5)下注阶段执行了看牌操作的用户直接out。
	r6)一局游戏结束后，上一局游戏的状态对当前游戏会造成一定影响，需要加入RNN层保留对前面几局的印象。(人经常会受到前面几局游戏的影响，从而影响对当前居面的判断)。

DQN:
	价值网络:
		根据当前的状态输出每一个状态的价值，利用TD_error作为损失函数。
		输出的值经过了缩放处理，限制在-1，1,这就要求对于奖励也要进行相应的缩放处理。
	策略:
		根据价值网络，每次都选择价值最大的动作。
AC:
	输出：
		当前玩家采取每一个动作的价值。	
		当前玩家采取每一个动作的概率分布。

	损失函数：
		XXXXXXXXXXXXXXXXXXXXXXXXXXXX，这是一个难点，XXXXXXXXXXXXXXXXXXXXXXX，谁有好的想法，XXXXXXXXXXXXXXXXXXXXXXXXX
训练:
	生成两个模型：A、B。
	每一个玩家都使用A来做出决策。
	每执行一个动作，更新B的参数。
	每玩10局，把B的参数迁移到A。
	重复s2-s3。







