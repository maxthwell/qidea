包含以下内容
分别使用DQN、PPO算法进行强化学习，玩一个大家都喜闻乐见的游戏：扎金花。

扎金花游戏的状态有以下属性：
	玩家个数N
	所有玩家（包含智能体）的历史动作集。
	当前智能体的手牌。

智能体可以使用的动作：
	看牌（在下注、弃牌，动作之前）
	下注（1，3，5）
	比牌（和谁比，拥有多少赌注）
	弃牌 （游戏结束）
价值：
	智能体采取一个动作后的期望收益。
游戏结束条件：
	除了一个玩家外，其他的都弃牌或者比牌出局。

玩法：
	s1)发牌
	s2)所有的玩家可以选择执行看牌操作
	s3)玩家1从（下注、比牌、弃牌）中选择一个动作执行。
	s4)重复s2-s3知道桌面上只有一个玩家
	s5)给与胜利的玩家奖励。
规则:
	r1)前N轮不允许比牌。
	r2)每一个玩家不知道对方的手牌。
	r3)选择看牌后不允许再执行看牌动作
	r4)在看牌阶段如果选择了下注、比牌、弃牌等动作直接被淘汰出局
	r5)下注阶段执行了看牌操作的用户直接out。

DQN:
	价值网络:
		根据当前的状态输出每一个状态的价值，利用TD_error作为损失函数。

AC模式:
	价值网络：
		智能体根据游戏系统的状态，输出期望价值价值V(s)，以及每一个动作的优势A(s,a)。利用TD_error作为损失函数
	策略网络：
		智能体根据当前的状态输出每一个执行每一个动作的概率，利用策略梯度算法进行优化。


